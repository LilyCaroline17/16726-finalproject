/home/ubuntu/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
/home/ubuntu/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/ubuntu/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
================================================================================
                                      Opts                                      
--------------------------------------------------------------------------------
                             image_size: 128                                    
                                   disc: dc                                     
                                    gen: cycle                                  
                             g_conv_dim: 32                                     
                             d_conv_dim: 32                                     
                                   norm: instance                               
                              init_type: naive                                  
                            train_iters: 50000                                  
                             batch_size: 16                                     
                            num_workers: 2                                      
                                     lr: 0.0001                                 
                                  beta1: 0.5                                    
                                  beta2: 0.999                                  
                           lambda_cycle: 10                                     
                                      X: ..                                     
                                    ext: *.png                                  
                        data_preprocess: vanilla                                
                        checkpoint_dir: checkpoints_pretrained_style_id         
                    sample_dir: output/pretrained/.._10vanilla_instance_dc_cycle_naive
                               log_step: 10                                     
                           sample_every: 100                                    
                       checkpoint_every: 1000                                   
                                    gpu: 0                                      
================================================================================
Loaded 97724 samples from ../labels
Models moved to GPU.
                 MODEL                
---------------------------------------
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=31, bias=True)
)
---------------------------------------
Checkpoints not found. Starting from scratch.
Iteration [   10/50000] | loss: 3.7627
Iteration [   20/50000] | loss: 3.5936
Iteration [   30/50000] | loss: 3.5547
Iteration [   40/50000] | loss: 3.6608
Iteration [   50/50000] | loss: 3.6082
Iteration [   60/50000] | loss: 3.6901
Iteration [   70/50000] | loss: 3.4645
Iteration [   80/50000] | loss: 3.5727
Iteration [   90/50000] | loss: 3.5299
Iteration [  100/50000] | loss: 3.6059
Iteration [  110/50000] | loss: 3.4135
Iteration [  120/50000] | loss: 3.4096
Iteration [  130/50000] | loss: 3.2220
Iteration [  140/50000] | loss: 3.5181
Iteration [  150/50000] | loss: 3.4894
Iteration [  160/50000] | loss: 3.5197
Iteration [  170/50000] | loss: 3.6258
Iteration [  180/50000] | loss: 3.5772
Iteration [  190/50000] | loss: 3.1803
Iteration [  200/50000] | loss: 3.4336
Iteration [  210/50000] | loss: 3.6566
Iteration [  220/50000] | loss: 3.4716
Iteration [  230/50000] | loss: 3.4035
Iteration [  240/50000] | loss: 3.1415
Iteration [  250/50000] | loss: 3.2851
Iteration [  260/50000] | loss: 3.3982
Iteration [  270/50000] | loss: 3.4301
Iteration [  280/50000] | loss: 3.3800
Iteration [  290/50000] | loss: 3.3294
Iteration [  300/50000] | loss: 3.4535
Iteration [  310/50000] | loss: 3.1946
Iteration [  320/50000] | loss: 3.1865
Iteration [  330/50000] | loss: 3.2821
Iteration [  340/50000] | loss: 3.1060
Iteration [  350/50000] | loss: 3.5244
Iteration [  360/50000] | loss: 3.3481
Iteration [  370/50000] | loss: 3.2482
Iteration [  380/50000] | loss: 3.3402
Iteration [  390/50000] | loss: 3.0913
Iteration [  400/50000] | loss: 3.3178
Iteration [  410/50000] | loss: 3.2438
Iteration [  420/50000] | loss: 3.2974
Iteration [  430/50000] | loss: 3.1960
Iteration [  440/50000] | loss: 3.2816
Iteration [  450/50000] | loss: 3.3436
Iteration [  460/50000] | loss: 3.1575
Iteration [  470/50000] | loss: 3.0555
Iteration [  480/50000] | loss: 3.1712
Iteration [  490/50000] | loss: 3.1516
Iteration [  500/50000] | loss: 2.9943
Iteration [  510/50000] | loss: 3.1130
Iteration [  520/50000] | loss: 3.3258
Iteration [  530/50000] | loss: 3.1159
Iteration [  540/50000] | loss: 2.9889
Iteration [  550/50000] | loss: 3.3472
Iteration [  560/50000] | loss: 2.9660
Iteration [  570/50000] | loss: 3.1980
Iteration [  580/50000] | loss: 2.8982
Iteration [  590/50000] | loss: 3.1755
Iteration [  600/50000] | loss: 3.2042
Iteration [  610/50000] | loss: 3.0044
Iteration [  620/50000] | loss: 3.1761
Iteration [  630/50000] | loss: 2.8997
Iteration [  640/50000] | loss: 2.9271
Iteration [  650/50000] | loss: 3.0951
Iteration [  660/50000] | loss: 3.0048
Iteration [  670/50000] | loss: 3.2926
Iteration [  680/50000] | loss: 2.9893
Iteration [  690/50000] | loss: 2.9805
Iteration [  700/50000] | loss: 3.1442
Iteration [  710/50000] | loss: 3.0133
Iteration [  720/50000] | loss: 3.1679
Iteration [  730/50000] | loss: 3.1606
Iteration [  740/50000] | loss: 3.0856
Iteration [  750/50000] | loss: 2.8888
Iteration [  760/50000] | loss: 3.1406
Iteration [  770/50000] | loss: 2.9011
Iteration [  780/50000] | loss: 2.9469
Iteration [  790/50000] | loss: 3.2602
Iteration [  800/50000] | loss: 2.9238
Iteration [  810/50000] | loss: 3.3565
Iteration [  820/50000] | loss: 3.2176
Iteration [  830/50000] | loss: 2.9785
Iteration [  840/50000] | loss: 2.8574
Iteration [  850/50000] | loss: 3.2351
Iteration [  860/50000] | loss: 2.9766
Iteration [  870/50000] | loss: 2.9314
Iteration [  880/50000] | loss: 2.9388
Iteration [  890/50000] | loss: 3.0434
Iteration [  900/50000] | loss: 3.0153
Iteration [  910/50000] | loss: 2.9835
Iteration [  920/50000] | loss: 2.8431
Iteration [  930/50000] | loss: 2.7307
Iteration [  940/50000] | loss: 2.7426
Iteration [  950/50000] | loss: 3.2192
Iteration [  960/50000] | loss: 3.0378
Iteration [  970/50000] | loss: 2.7720
Iteration [  980/50000] | loss: 2.9549
Iteration [  990/50000] | loss: 3.3084
Iteration [ 1000/50000] | loss: 2.8246
Validation | loss: 2.8585
Iteration [ 1010/50000] | loss: 3.0170
Iteration [ 1020/50000] | loss: 2.8242
Iteration [ 1030/50000] | loss: 3.2839
Iteration [ 1040/50000] | loss: 2.7954
Iteration [ 1050/50000] | loss: 2.9848
Iteration [ 1060/50000] | loss: 3.1669
Iteration [ 1070/50000] | loss: 3.2524
Iteration [ 1080/50000] | loss: 3.0098
Iteration [ 1090/50000] | loss: 2.9654
Iteration [ 1100/50000] | loss: 2.8675
Iteration [ 1110/50000] | loss: 2.8228
Iteration [ 1120/50000] | loss: 3.1858
Iteration [ 1130/50000] | loss: 3.0903
Iteration [ 1140/50000] | loss: 2.9349
Iteration [ 1150/50000] | loss: 2.8812
Iteration [ 1160/50000] | loss: 3.0980
Iteration [ 1170/50000] | loss: 3.0929
Iteration [ 1180/50000] | loss: 2.7655
Iteration [ 1190/50000] | loss: 2.9488
Iteration [ 1200/50000] | loss: 2.9461
Iteration [ 1210/50000] | loss: 2.9449
Iteration [ 1220/50000] | loss: 2.9957
Iteration [ 1230/50000] | loss: 2.8536
Iteration [ 1240/50000] | loss: 2.8645
Iteration [ 1250/50000] | loss: 2.8905
Iteration [ 1260/50000] | loss: 2.7925
Iteration [ 1270/50000] | loss: 2.9134
Iteration [ 1280/50000] | loss: 3.2498
Iteration [ 1290/50000] | loss: 2.8287
Iteration [ 1300/50000] | loss: 2.8056
Iteration [ 1310/50000] | loss: 3.0100
Iteration [ 1320/50000] | loss: 3.1668
Iteration [ 1330/50000] | loss: 2.8711
Iteration [ 1340/50000] | loss: 2.5637
Iteration [ 1350/50000] | loss: 2.6222
Iteration [ 1360/50000] | loss: 3.1389
Iteration [ 1370/50000] | loss: 2.9550
Iteration [ 1380/50000] | loss: 2.5493
Iteration [ 1390/50000] | loss: 2.8609
Iteration [ 1400/50000] | loss: 2.8901
Iteration [ 1410/50000] | loss: 2.9536
Iteration [ 1420/50000] | loss: 3.0117
Iteration [ 1430/50000] | loss: 2.6738
Iteration [ 1440/50000] | loss: 2.6164
Iteration [ 1450/50000] | loss: 2.9624
Iteration [ 1460/50000] | loss: 3.1956
Iteration [ 1470/50000] | loss: 2.7978
Iteration [ 1480/50000] | loss: 2.9791
Iteration [ 1490/50000] | loss: 2.9465
Iteration [ 1500/50000] | loss: 2.7010
Iteration [ 1510/50000] | loss: 2.9378
Iteration [ 1520/50000] | loss: 3.0752
Iteration [ 1530/50000] | loss: 2.6180
Iteration [ 1540/50000] | loss: 2.8784
Iteration [ 1550/50000] | loss: 2.7472
Iteration [ 1560/50000] | loss: 2.7968
Iteration [ 1570/50000] | loss: 2.9191
Iteration [ 1580/50000] | loss: 2.9083
Iteration [ 1590/50000] | loss: 2.8510
Iteration [ 1600/50000] | loss: 3.0101
Iteration [ 1610/50000] | loss: 3.0234
Iteration [ 1620/50000] | loss: 2.4726
Iteration [ 1630/50000] | loss: 2.7998
Iteration [ 1640/50000] | loss: 2.5572
Iteration [ 1650/50000] | loss: 2.6073
Iteration [ 1660/50000] | loss: 2.6522
Iteration [ 1670/50000] | loss: 2.5835
Iteration [ 1680/50000] | loss: 2.7599
Iteration [ 1690/50000] | loss: 2.6577
Iteration [ 1700/50000] | loss: 2.5228
Iteration [ 1710/50000] | loss: 2.6346
Iteration [ 1720/50000] | loss: 2.7882
Iteration [ 1730/50000] | loss: 2.7037
Iteration [ 1740/50000] | loss: 2.6601
Iteration [ 1750/50000] | loss: 2.7745
Iteration [ 1760/50000] | loss: 2.7500
Iteration [ 1770/50000] | loss: 3.0725
Iteration [ 1780/50000] | loss: 2.8873
Iteration [ 1790/50000] | loss: 2.5915
Iteration [ 1800/50000] | loss: 2.3663
Iteration [ 1810/50000] | loss: 2.8635
Iteration [ 1820/50000] | loss: 2.7354
Iteration [ 1830/50000] | loss: 2.7365
Iteration [ 1840/50000] | loss: 2.6101
Iteration [ 1850/50000] | loss: 2.5362
Iteration [ 1860/50000] | loss: 2.4313
Iteration [ 1870/50000] | loss: 2.9121
Iteration [ 1880/50000] | loss: 2.6286
Iteration [ 1890/50000] | loss: 2.5472
Iteration [ 1900/50000] | loss: 2.9803
Iteration [ 1910/50000] | loss: 2.6641
Iteration [ 1920/50000] | loss: 2.7911
Iteration [ 1930/50000] | loss: 2.7557
Iteration [ 1940/50000] | loss: 2.7844
Iteration [ 1950/50000] | loss: 2.7632
Iteration [ 1960/50000] | loss: 2.5761
Iteration [ 1970/50000] | loss: 2.7761
Iteration [ 1980/50000] | loss: 2.7596
Iteration [ 1990/50000] | loss: 2.7891
Iteration [ 2000/50000] | loss: 2.8310
