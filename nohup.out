/home/ubuntu/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/ubuntu/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
================================================================================
                                      Opts                                      
--------------------------------------------------------------------------------
                             image_size: 128                                    
                                   disc: dc                                     
                                    gen: cycle                                  
                             g_conv_dim: 32                                     
                             d_conv_dim: 32                                     
                                   norm: instance                               
                              init_type: naive                                  
                            train_iters: 50000                                  
                             batch_size: 16                                     
                            num_workers: 2                                      
                                     lr: 0.0001                                 
                                  beta1: 0.5                                    
                                  beta2: 0.999                                  
                           lambda_cycle: 10                                     
                                      X: ..                                     
                                    ext: *.png                                  
                        data_preprocess: vanilla                                
                        checkpoint_dir: checkpoints_pretrained_style_id         
                    sample_dir: output/pretrained/.._10vanilla_instance_dc_cycle_naive
                               log_step: 10                                     
                           sample_every: 100                                    
                       checkpoint_every: 800                                    
                                    gpu: 0                                      
================================================================================
Loaded 393345 samples from ../labels
Models moved to GPU.
                 MODEL                
---------------------------------------
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=31, bias=True)
)
---------------------------------------
Checkpoints not found. Starting from scratch.
Iteration [   10/50000] | loss: 0.6015
Iteration [   20/50000] | loss: 0.4542
Iteration [   30/50000] | loss: 0.3591
Iteration [   40/50000] | loss: 0.2962
Iteration [   50/50000] | loss: 0.2362
Iteration [   60/50000] | loss: 0.2185
Iteration [   70/50000] | loss: 0.1976
Iteration [   80/50000] | loss: 0.1833
Iteration [   90/50000] | loss: 0.1850
Iteration [  100/50000] | loss: 0.1691
Iteration [  110/50000] | loss: 0.1681
Iteration [  120/50000] | loss: 0.1438
Iteration [  130/50000] | loss: 0.1475
Iteration [  140/50000] | loss: 0.1516
Iteration [  150/50000] | loss: 0.1549
Iteration [  160/50000] | loss: 0.1536
Iteration [  170/50000] | loss: 0.1668
Iteration [  180/50000] | loss: 0.1485
Iteration [  190/50000] | loss: 0.1419
Iteration [  200/50000] | loss: 0.1337
Iteration [  210/50000] | loss: 0.1517
Iteration [  220/50000] | loss: 0.1510
Iteration [  230/50000] | loss: 0.1305
Iteration [  240/50000] | loss: 0.1430
Iteration [  250/50000] | loss: 0.1363
Iteration [  260/50000] | loss: 0.1350
Iteration [  270/50000] | loss: 0.1395
Iteration [  280/50000] | loss: 0.1502
Iteration [  290/50000] | loss: 0.1380
Iteration [  300/50000] | loss: 0.1358
Iteration [  310/50000] | loss: 0.1306
Iteration [  320/50000] | loss: 0.1417
Iteration [  330/50000] | loss: 0.1251
Iteration [  340/50000] | loss: 0.1326
Iteration [  350/50000] | loss: 0.1295
Iteration [  360/50000] | loss: 0.1409
Iteration [  370/50000] | loss: 0.1377
Iteration [  380/50000] | loss: 0.1298
Iteration [  390/50000] | loss: 0.1470
Iteration [  400/50000] | loss: 0.1297
Iteration [  410/50000] | loss: 0.1244
Iteration [  420/50000] | loss: 0.1289
Iteration [  430/50000] | loss: 0.1265
Iteration [  440/50000] | loss: 0.1449
Iteration [  450/50000] | loss: 0.1324
Iteration [  460/50000] | loss: 0.1341
Iteration [  470/50000] | loss: 0.1275
Iteration [  480/50000] | loss: 0.1428
Iteration [  490/50000] | loss: 0.1257
Iteration [  500/50000] | loss: 0.1305
Iteration [  510/50000] | loss: 0.1366
Iteration [  520/50000] | loss: 0.1275
Iteration [  530/50000] | loss: 0.1329
Iteration [  540/50000] | loss: 0.1358
Iteration [  550/50000] | loss: 0.1358
Iteration [  560/50000] | loss: 0.1224
Iteration [  570/50000] | loss: 0.1326
Iteration [  580/50000] | loss: 0.1324
Iteration [  590/50000] | loss: 0.1363
Iteration [  600/50000] | loss: 0.1307
Iteration [  610/50000] | loss: 0.1234
Iteration [  620/50000] | loss: 0.1327
Iteration [  630/50000] | loss: 0.1342
Iteration [  640/50000] | loss: 0.1292
Iteration [  650/50000] | loss: 0.1202
Iteration [  660/50000] | loss: 0.1222
Iteration [  670/50000] | loss: 0.1356
Iteration [  680/50000] | loss: 0.1202
Iteration [  690/50000] | loss: 0.1314
Iteration [  700/50000] | loss: 0.1477
Iteration [  710/50000] | loss: 0.1309
Iteration [  720/50000] | loss: 0.1428
Iteration [  730/50000] | loss: 0.1188
Iteration [  740/50000] | loss: 0.1384
Iteration [  750/50000] | loss: 0.1254
Iteration [  760/50000] | loss: 0.1312
Iteration [  770/50000] | loss: 0.1386
Iteration [  780/50000] | loss: 0.1392
Iteration [  790/50000] | loss: 0.1241
Iteration [  800/50000] | loss: 0.1357
Traceback (most recent call last):
  File "/mnt/hw5/final_proj/16726-finalproject/pretrained_style_identifier.py", line 345, in <module>
    main(opts)
  File "/mnt/hw5/final_proj/16726-finalproject/pretrained_style_identifier.py", line 260, in main
    training_loop(dataloader_X, validation_loader, opts)
  File "/mnt/hw5/final_proj/16726-finalproject/pretrained_style_identifier.py", line 240, in training_loop
    val_loss_total += val_loss.item()
NameError: name 'val_loss' is not defined
/home/ubuntu/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
/home/ubuntu/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/ubuntu/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
================================================================================
                                      Opts                                      
--------------------------------------------------------------------------------
                             image_size: 128                                    
                                   disc: dc                                     
                                    gen: cycle                                  
                             g_conv_dim: 32                                     
                             d_conv_dim: 32                                     
                                   norm: instance                               
                              init_type: naive                                  
                            train_iters: 50000                                  
                             batch_size: 16                                     
                            num_workers: 2                                      
                                     lr: 0.0001                                 
                                  beta1: 0.5                                    
                                  beta2: 0.999                                  
                           lambda_cycle: 10                                     
                                      X: ..                                     
                                    ext: *.png                                  
                        data_preprocess: vanilla                                
                        checkpoint_dir: checkpoints_pretrained_style_id         
                    sample_dir: output/pretrained/.._10vanilla_instance_dc_cycle_naive
                               log_step: 10                                     
                           sample_every: 100                                    
                       checkpoint_every: 800                                    
                                    gpu: 0                                      
================================================================================
Loaded 393345 samples from ../labels
Models moved to GPU.
                 MODEL                
---------------------------------------
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=31, bias=True)
)
---------------------------------------
Checkpoints not found. Starting from scratch.
Iteration [   10/50000] | loss: 0.6015
Iteration [   20/50000] | loss: 0.4542
Iteration [   30/50000] | loss: 0.3591
Iteration [   40/50000] | loss: 0.2962
Iteration [   50/50000] | loss: 0.2362
Iteration [   60/50000] | loss: 0.2185
Iteration [   70/50000] | loss: 0.1976
Iteration [   80/50000] | loss: 0.1833
Iteration [   90/50000] | loss: 0.1850
Iteration [  100/50000] | loss: 0.1691
Iteration [  110/50000] | loss: 0.1681
Iteration [  120/50000] | loss: 0.1438
Iteration [  130/50000] | loss: 0.1475
Iteration [  140/50000] | loss: 0.1516
Iteration [  150/50000] | loss: 0.1549
Iteration [  160/50000] | loss: 0.1536
Iteration [  170/50000] | loss: 0.1668
Iteration [  180/50000] | loss: 0.1485
Iteration [  190/50000] | loss: 0.1419
Iteration [  200/50000] | loss: 0.1337
Iteration [  210/50000] | loss: 0.1517
Iteration [  220/50000] | loss: 0.1510
Iteration [  230/50000] | loss: 0.1305
Iteration [  240/50000] | loss: 0.1430
Iteration [  250/50000] | loss: 0.1363
Iteration [  260/50000] | loss: 0.1350
Iteration [  270/50000] | loss: 0.1395
Iteration [  280/50000] | loss: 0.1502
Iteration [  290/50000] | loss: 0.1380
Iteration [  300/50000] | loss: 0.1358
Iteration [  310/50000] | loss: 0.1306
Iteration [  320/50000] | loss: 0.1417
Iteration [  330/50000] | loss: 0.1251
Iteration [  340/50000] | loss: 0.1326
Iteration [  350/50000] | loss: 0.1295
Iteration [  360/50000] | loss: 0.1409
Iteration [  370/50000] | loss: 0.1377
Iteration [  380/50000] | loss: 0.1298
Iteration [  390/50000] | loss: 0.1470
Iteration [  400/50000] | loss: 0.1297
Iteration [  410/50000] | loss: 0.1244
Iteration [  420/50000] | loss: 0.1289
Iteration [  430/50000] | loss: 0.1265
Iteration [  440/50000] | loss: 0.1449
Iteration [  450/50000] | loss: 0.1324
Iteration [  460/50000] | loss: 0.1341
Iteration [  470/50000] | loss: 0.1275
Iteration [  480/50000] | loss: 0.1428
Iteration [  490/50000] | loss: 0.1257
Iteration [  500/50000] | loss: 0.1305
Iteration [  510/50000] | loss: 0.1366
Iteration [  520/50000] | loss: 0.1275
Iteration [  530/50000] | loss: 0.1329
Iteration [  540/50000] | loss: 0.1358
Iteration [  550/50000] | loss: 0.1358
Iteration [  560/50000] | loss: 0.1224
Iteration [  570/50000] | loss: 0.1326
Iteration [  580/50000] | loss: 0.1324
Iteration [  590/50000] | loss: 0.1363
Iteration [  600/50000] | loss: 0.1307
Iteration [  610/50000] | loss: 0.1234
Iteration [  620/50000] | loss: 0.1327
Iteration [  630/50000] | loss: 0.1342
Iteration [  640/50000] | loss: 0.1292
Iteration [  650/50000] | loss: 0.1202
Iteration [  660/50000] | loss: 0.1222
Iteration [  670/50000] | loss: 0.1356
Iteration [  680/50000] | loss: 0.1202
Iteration [  690/50000] | loss: 0.1314
Iteration [  700/50000] | loss: 0.1477
Iteration [  710/50000] | loss: 0.1309
Iteration [  720/50000] | loss: 0.1428
Iteration [  730/50000] | loss: 0.1188
Iteration [  740/50000] | loss: 0.1384
Iteration [  750/50000] | loss: 0.1254
Iteration [  760/50000] | loss: 0.1312
Iteration [  770/50000] | loss: 0.1386
Iteration [  780/50000] | loss: 0.1392
Iteration [  790/50000] | loss: 0.1241
Iteration [  800/50000] | loss: 0.1357
Validation | loss: 0.1357
Iteration [  810/50000] | loss: 0.1322
Iteration [  820/50000] | loss: 0.1280
Iteration [  830/50000] | loss: 0.1224
Iteration [  840/50000] | loss: 0.1312
Iteration [  850/50000] | loss: 0.1406
Iteration [  860/50000] | loss: 0.1430
Iteration [  870/50000] | loss: 0.1330
Iteration [  880/50000] | loss: 0.1278
Iteration [  890/50000] | loss: 0.1254
Iteration [  900/50000] | loss: 0.1240
Iteration [  910/50000] | loss: 0.1204
Iteration [  920/50000] | loss: 0.1319
Iteration [  930/50000] | loss: 0.1279
Iteration [  940/50000] | loss: 0.1309
Iteration [  950/50000] | loss: 0.1306
Iteration [  960/50000] | loss: 0.1136
Iteration [  970/50000] | loss: 0.1253
Iteration [  980/50000] | loss: 0.1216
Iteration [  990/50000] | loss: 0.1303
Iteration [ 1000/50000] | loss: 0.1109
Iteration [ 1010/50000] | loss: 0.1350
Iteration [ 1020/50000] | loss: 0.1285
Iteration [ 1030/50000] | loss: 0.1369
Iteration [ 1040/50000] | loss: 0.1306
Iteration [ 1050/50000] | loss: 0.1287
Iteration [ 1060/50000] | loss: 0.1267
Iteration [ 1070/50000] | loss: 0.1218
Iteration [ 1080/50000] | loss: 0.1177
Iteration [ 1090/50000] | loss: 0.1365
Iteration [ 1100/50000] | loss: 0.1332
Iteration [ 1110/50000] | loss: 0.1367
Iteration [ 1120/50000] | loss: 0.1203
Iteration [ 1130/50000] | loss: 0.1377
Iteration [ 1140/50000] | loss: 0.1191
Iteration [ 1150/50000] | loss: 0.1186
Iteration [ 1160/50000] | loss: 0.1268
Iteration [ 1170/50000] | loss: 0.1104
Iteration [ 1180/50000] | loss: 0.1288
Iteration [ 1190/50000] | loss: 0.1216
Iteration [ 1200/50000] | loss: 0.1255
Iteration [ 1210/50000] | loss: 0.1063
Iteration [ 1220/50000] | loss: 0.1226
Iteration [ 1230/50000] | loss: 0.1207
Iteration [ 1240/50000] | loss: 0.1216
Iteration [ 1250/50000] | loss: 0.1291
Iteration [ 1260/50000] | loss: 0.1170
Iteration [ 1270/50000] | loss: 0.1210
Iteration [ 1280/50000] | loss: 0.1019
Iteration [ 1290/50000] | loss: 0.1152
Iteration [ 1300/50000] | loss: 0.1247
Iteration [ 1310/50000] | loss: 0.1414
Iteration [ 1320/50000] | loss: 0.1331
Iteration [ 1330/50000] | loss: 0.1147
Iteration [ 1340/50000] | loss: 0.1302
Iteration [ 1350/50000] | loss: 0.1249
Iteration [ 1360/50000] | loss: 0.1209
Iteration [ 1370/50000] | loss: 0.1199
Iteration [ 1380/50000] | loss: 0.1230
Iteration [ 1390/50000] | loss: 0.1306
Iteration [ 1400/50000] | loss: 0.1134
Iteration [ 1410/50000] | loss: 0.1306
Iteration [ 1420/50000] | loss: 0.1218
Iteration [ 1430/50000] | loss: 0.1232
Iteration [ 1440/50000] | loss: 0.1363
Iteration [ 1450/50000] | loss: 0.1365
Iteration [ 1460/50000] | loss: 0.1205
Iteration [ 1470/50000] | loss: 0.1215
Iteration [ 1480/50000] | loss: 0.1348
Iteration [ 1490/50000] | loss: 0.1269
Iteration [ 1500/50000] | loss: 0.1310
Iteration [ 1510/50000] | loss: 0.1255
Iteration [ 1520/50000] | loss: 0.1162
Iteration [ 1530/50000] | loss: 0.1201
Iteration [ 1540/50000] | loss: 0.1249
Iteration [ 1550/50000] | loss: 0.1138
Iteration [ 1560/50000] | loss: 0.1243
Iteration [ 1570/50000] | loss: 0.1283
Iteration [ 1580/50000] | loss: 0.1333
Iteration [ 1590/50000] | loss: 0.1168
Iteration [ 1600/50000] | loss: 0.1355
Validation | loss: 0.1355
Iteration [ 1610/50000] | loss: 0.1165
Iteration [ 1620/50000] | loss: 0.1190
Iteration [ 1630/50000] | loss: 0.1122
Iteration [ 1640/50000] | loss: 0.1185
Iteration [ 1650/50000] | loss: 0.1237
Iteration [ 1660/50000] | loss: 0.1285
Iteration [ 1670/50000] | loss: 0.1212
Iteration [ 1680/50000] | loss: 0.1347
Iteration [ 1690/50000] | loss: 0.1227
Iteration [ 1700/50000] | loss: 0.1240
Iteration [ 1710/50000] | loss: 0.1175
Iteration [ 1720/50000] | loss: 0.1336
Iteration [ 1730/50000] | loss: 0.1210
Iteration [ 1740/50000] | loss: 0.1059
Iteration [ 1750/50000] | loss: 0.1048
Iteration [ 1760/50000] | loss: 0.1354
Iteration [ 1770/50000] | loss: 0.1085
Iteration [ 1780/50000] | loss: 0.1375
Iteration [ 1790/50000] | loss: 0.1068
Iteration [ 1800/50000] | loss: 0.1271
Iteration [ 1810/50000] | loss: 0.1169
Iteration [ 1820/50000] | loss: 0.1292
Iteration [ 1830/50000] | loss: 0.1084
Iteration [ 1840/50000] | loss: 0.1064
Iteration [ 1850/50000] | loss: 0.1241
Iteration [ 1860/50000] | loss: 0.1106
Iteration [ 1870/50000] | loss: 0.1192
Iteration [ 1880/50000] | loss: 0.1233
Iteration [ 1890/50000] | loss: 0.1324
Iteration [ 1900/50000] | loss: 0.1196
Iteration [ 1910/50000] | loss: 0.1216
Iteration [ 1920/50000] | loss: 0.1159
Iteration [ 1930/50000] | loss: 0.1202
Iteration [ 1940/50000] | loss: 0.1162
Iteration [ 1950/50000] | loss: 0.1046
Iteration [ 1960/50000] | loss: 0.1153
Iteration [ 1970/50000] | loss: 0.1126
Iteration [ 1980/50000] | loss: 0.1171
Iteration [ 1990/50000] | loss: 0.1120
Iteration [ 2000/50000] | loss: 0.1292
Iteration [ 2010/50000] | loss: 0.1156
Iteration [ 2020/50000] | loss: 0.1140
Iteration [ 2030/50000] | loss: 0.1010
Iteration [ 2040/50000] | loss: 0.1011
Iteration [ 2050/50000] | loss: 0.1047
Iteration [ 2060/50000] | loss: 0.1208
Iteration [ 2070/50000] | loss: 0.1124
Iteration [ 2080/50000] | loss: 0.1038
Iteration [ 2090/50000] | loss: 0.1129
Iteration [ 2100/50000] | loss: 0.1207
Iteration [ 2110/50000] | loss: 0.1141
Iteration [ 2120/50000] | loss: 0.1115
Iteration [ 2130/50000] | loss: 0.1138
Iteration [ 2140/50000] | loss: 0.1061
Iteration [ 2150/50000] | loss: 0.1064
Iteration [ 2160/50000] | loss: 0.1261
Iteration [ 2170/50000] | loss: 0.1155
Iteration [ 2180/50000] | loss: 0.1205
Iteration [ 2190/50000] | loss: 0.1045
Iteration [ 2200/50000] | loss: 0.1164
Iteration [ 2210/50000] | loss: 0.1110
Iteration [ 2220/50000] | loss: 0.1170
Iteration [ 2230/50000] | loss: 0.1230
Iteration [ 2240/50000] | loss: 0.1163
Iteration [ 2250/50000] | loss: 0.1140
Iteration [ 2260/50000] | loss: 0.1022
Iteration [ 2270/50000] | loss: 0.1182
Iteration [ 2280/50000] | loss: 0.1159
Iteration [ 2290/50000] | loss: 0.1245
Iteration [ 2300/50000] | loss: 0.1044
Iteration [ 2310/50000] | loss: 0.1143
Iteration [ 2320/50000] | loss: 0.1155
Iteration [ 2330/50000] | loss: 0.1091
Iteration [ 2340/50000] | loss: 0.1136
Iteration [ 2350/50000] | loss: 0.1303
Iteration [ 2360/50000] | loss: 0.1162
Iteration [ 2370/50000] | loss: 0.1135
Iteration [ 2380/50000] | loss: 0.1290
Iteration [ 2390/50000] | loss: 0.1050
Iteration [ 2400/50000] | loss: 0.1184
