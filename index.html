<!DOCTYPE html>
<html>
<head>
   <meta charset="UTF-8">
   <title>Final Project</title>
   <style>
      body {
         font-family: "Optima", sans-serif;
         color: #333;
         background-color: #E6E6FA;
         margin: 0;
         padding: 0;
      }

      h1 {
         text-align: center;
         font-size: 2.8em;
         margin: 30px 0 10px 0;
         color: #483D8B;
      }

      h2 {
         text-align: center;
         background-color: #D8BFD8;
         padding: 12px;
         border-radius: 8px;
         margin: 40px auto 20px auto;
         width: 80%;
         color: #483D8B;
         font-size: 1.8em;
         box-shadow: 0 2px 6px rgba(106, 90, 205, 0.2);
      }

      .section-container {
         text-align: center;
         background-color: white;
         border-radius: 12px;
         margin: 20px auto;
         width: 85%;
      }

      p {
         text-align: left;
         font-size: 18px;
         padding: 0 100px;
         line-height: 1.6;
         color: #333;
      }

      .image-wrapper {
         display: flex;
         flex-direction: column;
         gap: 20px;
         margin-top: 20px;
         box-shadow: 0px 4px 10px rgba(106, 90, 205, 0.2);
      }

      .image-row {
         display: flex;
         justify-content: center;
         gap: 25px;
         flex-wrap: wrap;
      }      

      .image-container {
         text-align: center;
         max-width: 600px;
         margin: 0 auto; /* center horizontally */
      }
      

      img {
         width: 100%;
         max-width: 500px;
         height: auto;
         display: block;
         margin: 0 auto; /* center the image */
         border-radius: 8px;
         transition: transform 0.3s ease-in-out;
         box-shadow: 0 4px 8px rgba(106, 90, 205, 0.2);
      }
      

      img:hover {
         transform: scale(1.03);
      }

      figcaption {
         margin-top: 8px;
         font-size: 16px;
         font-weight: bold;
         color: #483D8B;
      }

      .title-container {
         text-align: center;
         background-color: #DDA0DD;
         width: 85%;
         padding: 20px;
         border-radius: 12px;
         margin: 0 auto 30px auto;
         box-shadow: 0px 4px 10px rgba(186, 85, 211, 0.3);
      }

      .title-container p {
         margin-top: 5px;
         text-align: center;
         font-size: 18px;
         color: #444;
      }

      .styled-table {
         width: 85%;
         margin: 20px auto;
         border-collapse: collapse;
         box-shadow: 0px 4px 10px rgba(106, 90, 205, 0.2);
         background: white;
         text-align: center;
         border-radius: 12px;
         overflow: hidden;
      }

      .styled-table th, .styled-table td {
         padding: 15px;
         border: 1px solid #D8BFD8;
      }

      .styled-table th {
         background-color: #E6E6FA;
         font-size: 1.2em;
         color: #6A5ACD;
      }

      .styled-table img {
         width: 100px;
         height: auto;
         border-radius: 8px;
         transition: transform 0.3s ease-in-out;
      }

      footer {
         text-align: center;
         font-size: 14px;
         color: #999;
         margin: 40px 0;
      }
   </style>
</head>
<body>
   <div class="title-container">
      <h1>FINAL PROJECT</h1>
      <p>By Flora Cheng and Emily Guo<br>
         16-726 Learning-Based Image Synthesis (Spring 2025)</p>
   </div>

   <h2>Introduction</h2>
   <p>
      In this project, our goal was to create a model that, given a photo of a face and a desired hairstyle, generates an image of the original face with the new hairstyle. This was inspired by prior work, including <i>Learning to Generate and Edit Hairstyles</i> and 
      <a href="https://psh01087.github.io/K-Hairstyle/">K-Hairstyle: A Large-scale Korean hairstyle dataset</a>. 
      Our goal was to achieve similar results with less computation and smaller image sizes.
      <br><br>
      The dataset we used was from the K-Hairstyle paper, containing approximately 500k labeled images of 31 hairstyles.
   </p>
   <div class="image-wrapper">
      <div class="image-container">
         <img src="structure.png" style="width:900px;">
         <figcaption>GAN Structure Used</figcaption>
      </div>
   </div>

   <h2>Part 1: Naive Model</h2>
   <p>        
      In the first part of the project, we used the architecture done in HW3 as a basis. We used the Generator and Discriminator classes we created in that homework with updates 
      to accommodate our one-hot style vector. The updated generator takes in an image and a hot vector as input. The encoder processes the image and generates a latent, which is 
      combined with the style vector. The resnet block decodes it back inito an image. The idea is that if the input image is given the original (correct) one-hot style vector that 
      matches the image, the generator will return the input image. We also re-encode the image (get latent) with the original style vector to get the original input image (used for cycle loss). 
      A style identifier is created to make sure that the images match the style that they are supposed to. It's similar to hw3 with convolution layers, then blocks of resnet blocks 
      with average pools that halve the image size to 4x4. Then, we apply a linear and relu layer so that we get a 1 hot vector at the very end. The style loss measures how well the 
      style identifier classifies the images into the target style. 

      What we discovered is that our results weren't very good, and that our generated images for each of the one-hot vectors were the same image, suggesting that our style identifier is the issue. 
      This means that it's not accurately predicting the style of images (as we discovered it was terribly wrong on even the training data). We also tried playing around with weights 
      but the results were not significantly different. The results are below. 
   </p>
   <div class="image-wrapper">
      <div class="image-row">
         <div class="image-container">
            <img src="naiveLoss.png" alt="Naive Loss Graph">
            <figcaption>Naive Model Style Identifier Loss</figcaption>
         </div>
         <div class="image-container">
            <img src="naiveResults.png" alt="Naive Model Results">
            <figcaption>Naive Results after 10000 Iterations</figcaption>
         </div>
      </div>
   </div>

   <h2>Part 2: Pretrained model</h2>
   <p>
      In the second part of this project, we tried a pretrained model, hoping that it would be better at extracting features!
      We changed the loss from L1/L2 to Binary Cross-Entropy. Once again, we seem to run across the same issue. It seems that 
      our style identifier is failing us (which is not allowing the generator to be properly trained to actually generate 
      hairstyles that we want). 
   </p>
   <div class="image-wrapper">
      <div class="image-row">
         <div class="image-container">
            <img src="pretrainedLoss.png" alt="Pretrained Loss Graph">
            <figcaption>Pretrained Model Style Identifier Loss</figcaption>
         </div>
         <div class="image-container">
            <img src="pretrainedResults.png" alt="Pretrained Results">
            <figcaption>Pretrained Results after 10000 Iterations</figcaption>
         </div>
      </div>
   </div>

   <h2>Part 3: CLIP</h2>
   <p>
      We considered the possibility that the training data might not be large enough or consistent enough for models to pick up features, 
      so we're using CLIP, which already has embeddings. so that it's learning straight from scratch and so that it can use what it has 
      already to give us better style identifying (and style loss using cosine similarity loss)
   </p>
   <div class="image-wrapper">
      <div class="image-row">
         <div class="image-container">
            <img src="clipLoss.png" alt="CLIP Loss Graph">
            <figcaption>CLIP Model Style Identifier Loss</figcaption>
         </div>
         <div class="image-container">
            <img src="clipPartialRes2500.png" alt="CLIP Results">
            <figcaption>CLIP Results after 2500 Iterations</figcaption>
         </div>
      </div>
   </div>

   <h2>Part 4: Cutting Down on Dataset</h2>
   <p>
      We realized that we were training on the entire hairstyle dataset, which includes images of hairstyles from all sides, and the goal 
      of our hair transfer is to transfer these hairstyles onto a person, which would typically involve their face as an image of a hairstyle 
      from the back wouldn't let the user know how it would actually look on them (not really any defining features to determine that that person 
      is who they are), so we decided to limit the data to only use relatively front facing images with plus minus 45 degrees angle so that the 
      facial features are visible which will be similar to our user input images where they send an image of themselves in. This ended up reducing 
      our dataset size to 97724, which is about 25% of our total dataset (validation taken from this dataset as well which is removed here), which 
      means that we lost a large portion of data that could be useful to train our models. 
   </p>

   <h2>Application</h2>
   <p>
      Our final pipeline is as follows: <b>face input</b> → <i>blur face</i> → <b>generate hair</b> → <i>transplant face</i> → <b>output image</b>.
      When given an input image of a person's face, we start off by blurring the face of the image, which becomes our input image into the pre-trained 
      Generator from our trainings, along with a style vector of what that person wishes their hairstyle to be. The generated image is based on the 
      blurred face, so we want to add the face back in. If we do that, there may have been aspects in the generation process that makes the coloring, 
      contrasting, lighting, etc. different so we use poisson blend from homework 2 to make the input face back into the generated hairstyle look more 
      normal. 

      For the blurred face, we used an existing face detection software called get_frontal_face_detector from the dlib library to detect faces so that 
      we're accurately blurring the face of our person instead of having a set location blurred for an image. 
   </p>
   <div class="image-wrapper">
      <div class="image-row">
         <div class="image-container">
            <img src="test.jpg" style="width:200px;">
            <figcaption>Face Image</figcaption>
         </div>
         <div class="image-container">
            <img src="blurredInput.jpg" style="width:200px;">
            <figcaption>Blurred Face</figcaption>
         </div>
         <div class="image-container">
            <img src="generatorRes.jpg" style="width:200px;">
            <figcaption>Generated Hair</figcaption>
         </div>
         <div class="image-container">
            <img src="transplantRes.jpg" style="width:200px;">
            <figcaption>Transplanted Face</figcaption>
         </div>
         <div class="image-container">
            <img src="finalRes.jpg" style="width:200px;">
            <figcaption>Final Output</figcaption>
         </div>
      </div>
   </div>

   <h2>Final Results</h2>
   <p>
      We were able to demonstrate that CLIP-based embeddings and better architecture choices significantly improved visual fidelity and consistency.
      However, our results seem to be suffering from our style identifier, which is resulting in not having different generate images (besides the 
      slight change in the hair part as you see above). Future work would involve more existing models for the style identifier to determine how that 
      could improve our results. 
   </p>
</body>
</html>
