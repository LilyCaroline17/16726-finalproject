<!DOCTYPE html>
<html>
   <head>
      <meta charset="UTF-8">
      <title>Final Project</title>
      <style>
         body {
         font-family: "Optima", sans-serif;
         color: #333;
         background-color: #E6E6FA;
         margin: 0;
         padding: 0;
         }
         h1 {
         text-align: center;
         font-size: 2.8em;
         margin: 30px 0 10px 0;
         color: #483D8B;
         }
         h2 {
         text-align: center;
         background-color: #D8BFD8;
         padding: 12px;
         border-radius: 8px;
         margin: 40px auto 20px auto;
         width: 80%;
         color: #483D8B;
         font-size: 1.8em;
         box-shadow: 0 2px 6px rgba(106, 90, 205, 0.2);
         }
         .section-container {
         text-align: center;
         background-color: white;
         border-radius: 12px;
         margin: 20px auto;
         width: 85%;
         }
         p {
         text-align: left;
         font-size: 18px;
         padding: 0 100px;
         line-height: 1.6;
         color: #333;
         }
         ul {

         font-size: 18px;
         padding: 0 100px;
         line-height: 1.6;
         color: #333;
         }
         .image-wrapper {
         display: flex;
         flex-direction: column;
         gap: 20px;
         margin-top: 20px;
         box-shadow: 0px 4px 10px rgba(106, 90, 205, 0.2);
         }
         .image-row {
         display: flex;
         justify-content: center;
         gap: 25px;
         flex-wrap: wrap;
         }    
         .image-container {
         text-align: center;
         /* max-width: 1900px; */
         margin: 0 auto; /* center horizontally */
         }
         img {
         width: 100%;
         max-width: 800px;
         height: auto;
         display: block;
         margin: 0 auto; /* center the image */
         border-radius: 8px;
         transition: transform 0.3s ease-in-out;
         box-shadow: 0 4px 8px rgba(106, 90, 205, 0.2);
         }
         img:hover {
         transform: scale(1.03);
         }
         figcaption {
         margin-top: 8px;
         font-size: 16px;
         font-weight: bold;
         color: #483D8B;
         }
         .title-container {
         text-align: center;
         background-color: #DDA0DD;
         width: 85%;
         padding: 20px;
         border-radius: 12px;
         margin: 0 auto 30px auto;
         box-shadow: 0px 4px 10px rgba(186, 85, 211, 0.3);
         }
         .title-container p {
         margin-top: 5px;
         text-align: center;
         font-size: 18px;
         color: #444;
         }
         .styled-table {
         width: 85%;
         margin: 20px auto;
         border-collapse: collapse;
         box-shadow: 0px 4px 10px rgba(106, 90, 205, 0.2);
         background: white;
         text-align: center;
         border-radius: 12px;
         overflow: hidden;
         }
         .styled-table th, .styled-table td {
         padding: 15px;
         border: 1px solid #D8BFD8;
         }
         .styled-table th {
         background-color: #E6E6FA;
         font-size: 1.2em;
         color: #6A5ACD;
         }
         .styled-table img {
         width: 100px;
         height: auto;
         border-radius: 8px;
         transition: transform 0.3s ease-in-out;
         }
         footer {
         text-align: center;
         font-size: 14px;
         color: #999;
         margin: 40px 0;
         }
      </style>
   </head>
   <body>
      <div class="title-container">
         <h1>Final Project: Hair Style Transfer</h1>
         <p>By Flora Cheng and Emily Guo<br>
            16-726 Learning-Based Image Synthesis (Spring 2025)
         </p>
      </div>
      <h2>Introduction</h2>
      <p>
         In this project, our goal was to create a model that, given a photo of a face and a desired hairstyle, generates an image of the original face with the new hairstyle. This was inspired by prior work, including <a href = "https://yanweifu.github.io/papers/hairstyle_v_14_weidong.pdf">Learning to Generate and Edit Hairstyles</a> and
         <a href="https://psh01087.github.io/K-Hairstyle/">K-Hairstyle: A Large-scale Korean hairstyle dataset</a>.
         Our goal was to achieve similar results with less computation and smaller image sizes.
         <br><br>      You can find our complete code on <a href="https://github.com/LilyCaroline17/16726-finalproject/">Hair Transfer</a>.
         The dataset we used was from the K-Hairstyle paper, containing approximately 500k labeled images of 31 hairstyles. These 31 hairstyles (translated) are: {'Hershey', 'Parted', 'Build', 'Air', 'other female', 'As',
         'Comma', 'Tassel', 'other male', 'Hippi', 'Loop', 'Short female',
         'Body', 'One-block dandy', 'Soft two-block dandy', 'Bob', 'Pleats',
         'See-through dandy', 'Shadow', 'Misty', 'Dandy', 'Spin swallow',
         'Pomade', 'Short male', 'Bonnie', 'other layered', 'One length',
         'Short bob', 'Leaf', 'Regent', 'Baby'}.
      </p><tbody>
         <div class="image-row">
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/허쉬.jpg"  style="width:200px" alt=
         "Hershey" style="width:200px">
               <figcaption>허쉬 (Hershey) — 34,040</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/댄디otherangle.jpg"  style="width:200px" alt=
         "Dandy">
               <figcaption>댄디 (Dandy) — 7,798</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/빌드otherangle.jpg"  style="width:200px" alt=
         "Build">
               <figcaption>빌드 (Build) — 23,617</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/가르마.jpg"  style="width:200px" alt=
         "Parted">
               <figcaption>가르마 (Parted) — 12,510</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/남자일반숏otherangle.jpg"  style="width:200px" alt=
         "Short male">
               <figcaption>남자일반숏 (Short male) — 12,577</figcaption>
             </div>
           <!-- </td> -->
        </div>
         <div class="image-row">
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/테슬otherangle.jpg"  style="width:200px" alt=
         "Tassel">
               <figcaption>테슬 (Tassel) — 2,926</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/기타여자스타일.jpg"  style="width:200px" alt=
         "Other female">
               <figcaption>기타여자스타일 (Other female) — 2,964</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/쉼표.jpg"  style="width:200px" alt=
         "Comma">
               <figcaption>쉼표 (Comma) — 2,401</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/여자일반숏.jpg"  style="width:200px" alt=
         "Short female">
               <figcaption>여자일반숏 (Short female) — 5,527</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/포마드otherangle.jpg"  style="width:200px" alt=
         "Pomade">
               <figcaption>포마드 (Pomade) — 7,970</figcaption>
             </div>
           <!-- </td> -->
        </div>
         <div class="image-row">
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/보브otherangle.jpg"  style="width:200px" alt=
         "Bob">
               <figcaption>보브 (Bob) — 20,811</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/히피otherangle.jpg"  style="width:200px" alt=
         "Hippi">
               <figcaption>히피 (Hippi) — 13,455</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/미스티.jpg"  style="width:200px" alt=
         "Misty">
               <figcaption>미스티 (Misty) — 6,161</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/플리츠otherangle.jpg"  style="width:200px" alt=
         "Pleats">
               <figcaption>플리츠 (Pleats) — 15,370</figcaption>
             </div>
           <!-- </td> -->
           <!-- <!-- <td> --> -->
             <div class="image-container">
               <img src="sample_images/애즈.jpg"  style="width:200px" alt=
         "As">
               <figcaption>애즈 (As) — 8,143</figcaption>
             </div>
           <!-- </td> -->
        </div>
         <div class="image-row">
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/시스루댄디.jpg"  style="width:200px" alt=
         "See-through dandy">
               <figcaption>시스루댄디 (See-through dandy) — 4,981</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/리프.jpg"  style="width:200px" alt=
         "Leaf">
               <figcaption>리프 (Leaf) — 7,276</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/에어.jpg"  style="width:200px" alt=
         "Air">
               <figcaption>에어 (Air) — 29,418</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/바디otherangle.jpg"  style="width:200px" alt=
         "Body">
               <figcaption>바디 (Body) — 20,810</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/스핀스왈로.jpg"  style="width:200px" alt=
         "Spin swallow">
               <figcaption>스핀스왈로 (Spin swallow) — 4,936</figcaption>
             </div>
           <!-- </td> -->
        </div>
         <div class="image-row">
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/소프트투블럭댄디.jpg"  style="width:200px" alt=
         "Soft two-block dandy">
               <figcaption>소프트투블럭댄디 (Soft two-block dandy) — 7,446</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/원블럭댄디.jpg"  style="width:200px" alt=
         "One-block dandy">
               <figcaption>원블럭댄디 (One-block dandy) — 3,869</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/원랭스.jpg"  style="width:200px" alt=
         "One length">
               <figcaption>원랭스 (One length) — 34,040</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/기타레이어드otherangle.jpg"  style="width:200px" alt=
         "Other layered">
               <figcaption>기타레이어드 (Other layered) — 48,240</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/루프.jpg"  style="width:200px" alt=
         "Loop">
               <figcaption>루프 (Loop) — 2,544</figcaption>
             </div>
           <!-- </td> -->
        </div>
         <div class="image-row">
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/기타남자스타일.jpg"  style="width:200px" alt=
         "Other male">
               <figcaption>기타남자스타일 (Other male) — 11,242</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/베이비otherangle.jpg"  style="width:200px" alt=
         "Baby">
               <figcaption>베이비 (Baby) — 3,076</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/리젠트.jpg"  style="width:200px" alt=
         "Regent">
               <figcaption>리젠트 (Regent) — 9,837</figcaption>
             </div>
           <!-- </td> -->
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/보니.jpg"  style="width:200px" alt=
         "Bonnie">
               <figcaption>보니 (Bonnie) — 15,469</figcaption>
             </div> 
             <div class="image-container">
               <img src="sample_images/쉐도우otherangle.jpg"  style="width:200px" alt=
         "Shadow">
               <figcaption>쉐도우 (Shadow) — 10,298</figcaption>
             </div>
           <!-- </td> -->
        </div>
         <div class="image-row">
           <!-- <td> -->
             <div class="image-container">
               <img src="sample_images/숏단발otherangle.jpg"  style="width:200px" alt=
         "Short bob">
               <figcaption>숏단발 (Short bob) — 13,614</figcaption>
             </div>
           <!-- </td> -->
        </div
       </tbody>
        
      <p>
         The labels were given in Korean, the names next to it are the translated titles.
         <br>
         To build such an image generator, we needed to implement an image generation model. This was done using an GAN architecture as the input data we had wasn't 1-1 in changing a person's hairstyle. <br> The GAN architecture was built off of cycle GAN from HW3. We used the Generator and Discriminator classes with updates to accommodate conditioning with a one-hot style vector, such that the GAN model is able to generate different images using different vectors.<br>
         The updated generator takes in an image and a hot vector as input. The encoder processes the image and generates a latent representation of the image. This latent vector is combined with the style vector before being passed through the resnet block and decoder to generate an image.
      </p>
      <div class="image-container">
         <img src="images/structure.png" style="width:200%">
         <figcaption>GAN Structure Used</figcaption>
      </div>
      <p>
         To enforce the generated images to be similar to the original input image as well as have the correct style, we used the following losses:
     
      </p>
      <ul>
         <li>Realism loss <br>
            From typical GAN training, a goal of the generator is to generate realistic images. This is done by training the generator and discriminators as adversaries. The discriminator’s loss is found in how well it can differentiate generated images from real images. The generator’s realism loss is found by how much it can trick the discriminator. 
         </li>
         <li>Reconstruction loss<br>
            If the generator was ran on a generated output (created with a different hairstyle) and given the original (correct) one-hot style vector, it’s expected that the generator will return an output that closely resembles the input image. This cycle-loss would help enforce that the generated images still resemble the original image, such that it looks like the original user with a different hairstyle and that the end result isn’t someone else.
         </li>
         <li>Style loss<br>
            A style identifier is trained beforehand to identify each hairstyle type from the dataset. Then during training, the generator’s output is passed through the identifier to determine if the generated output has the target style. This will help enforce the generator to generate images based on the input style vector. 
         </li>
      </ul>
      <h2>Part 1: Style Identifier from Scratch</h2>
      <p>      
         The first attempt for creating the generator was with a style identifier that was trained from scratch. This model was built to with a similar structure to the generator model from HW3 as well as StarGANv2: convolution layers, then multiple resnet blocks with average pools that halve the image dimensions down to 4x4. Then, a linear and relu layer are applied to get a 1 hot style vector at the very end. This model was ran with 100k iterations over the dataset.<br>
      </p>
      <div class="image-container">
         <img src="images/naiveLoss.png"  alt=
         "Naive Loss Graph" style="width:800px">
         <figcaption>Naive Model Style Identifier Loss</figcaption>
      </div>
      <p>We can see that the loss ended up decreasing a bit over the iterations however the loss could go down further as the training and validation loss have not diverged yet. </p>
      <div class="image-container">
         <img src="images/naiveResults.png" alt=
         "Naive Model Results" style="width:800px">
         <figcaption>Naive Results after 10000 Iterations</figcaption>
      </div>
      <p>What we discovered is that our generated results weren't very distinct. This suggests that the weights of the GAN training for cycle and style loss were off. Further investigation also uncovered that our style identifier was not accurately predicting the style of images (as we discovered it wasn’t able to even correctly identify styles of the training data images).</p>
      <h2>Part 2: Pretrained model</h2>
      <p>
         In the second part of this project, we tried a pretrained model, hoping that it’s pretrained weights and structure would be better at extracting features that may assist in hairstyle identification!   
      </p>
      <div class="image-container">
         <img src="images/pretrainedLoss.png"  alt=
         "Pretrained Loss Graph">
         <figcaption>Pretrained Model Style Identifier Loss</figcaption>
      </div>
      <p> We can see that the training loss drops in the beginning with a pretrained model and plateaus much faster after approximately 500 iterations.</p>
      <div class="image-container">
         <img src="images/pretrainedResults.png"  alt=
         "Pretrained Results">
         <figcaption>Pretrained Results after 10000 Iterations</figcaption>
      </div>
      <p> However, based on the results again, we seem to run across the same issue. It seems that our style identifier is failing us (which is not allowing the generator to be properly trained to actually generate different hairstyles that we want).</p>
      <h2>Part 3: CLIP</h2>
      <p>
         We considered the possibility that the training data might not be large enough or consistent enough for models to pick up features, so we're used CLIP, which already has text and image embeddings. Rather than learning straight from scratch or off the dataset that may cause the identifier model confusion, we will use the CLIP embeddings of the image style labels as well as the images to find the style loss. This was ran for 2500 iterations due to memory constraints.
      </p> 
         <div class="image-container">
            <img src="images/clipPartialRes2500.png" alt=
         "CLIP Results">
            <figcaption>CLIP Results after 2500 Iterations</figcaption>
         </div> 
      <p>
         We can see that because we've run it only for a quarter of the iterations we usually do, it is quite pixelated and not far enough in the learning process. However it is also quite evident that our style identifier is still insufficient as all output images are too similar.
      </p>
      <h2>Part 4: Cutting Down on Dataset</h2>
      <p>
         With further investigation on the model results, we considered some things that may be resulting in such output images. <br>
         
         First we considered if training on the entire hairstyle dataset, which includes images of hairstyles from all angles, would throw off the models. Considering that the goal of our hair transfer is to transfer these hairstyles onto an input image of a person, which would typically involve making their face visible. So we decided to limit the data to only use relatively front-facing images with plus minus 45 degrees angle, such that the model may be trained on a smaller range of images. This ended up reducing our dataset size to 97724, which is about 25% of the total dataset (validation taken from this dataset as well which is removed here), which means that we lost a large portion of data that could be useful to train our models. <br>
         We also considered changing the lambda cycle value, which changes the weight of the cycle consistency loss. A higher lambda cycle means a more accurate reconstruction as the reconstruction loss is weighted higher. Originally the models were ran with a reconstruction weight of 10 and a style weight of 10. With further investigation, we found there was no significant difference in the results as shown below, only that lambda cycle = 3 had a darker hair color, suggesting the more realistic hair than the other.
      </p>  
            <div class="image-container">
               <img src="images/front310Results.png"  alt=
         "Pretrained lambda cycle = 3 Results">
               <figcaption>Using Front View Results after 10000 Iterations with Lambda Cycle = 3</figcaption>
            </div>
            <div class="image-container">
               <img src="images/front1010Results.png" alt=
         "Pretrained lambda cycle = 10 Results">
               <figcaption>Using Front View Results after 10000 Iterations with Lambda Cycle = 10</figcaption>
            </div> 
      <h2>Application</h2>
      <p>
         Our final pipeline is as follows: </p>
         <center><b>face input</b> → <i>blur face</i> → <b>generate hair</b> → <i>transplant face</i> → <b>output image</b></center>
<p>         When given an input image of a person's face, we start off by blurring the face of the image. This is because the dataset consists of images with blurred faces for privacy reasons. Thus we blur the image to input into the pre-trained Generator from our trainings, along with a style vector of what that person wishes their hairstyle to be. The generated image is based on the blurred face, so we want to add the face back in. Doing it naively may result in a harsh edge as the generation process may have made the coloring, contrasting, lighting, etc. different. So we use poisson blend from homework 2 to make the input face back into the generated hairstyle look more normal. For the blurred face, we used an existing face detection software called get_frontal_face_detector from the dlib library to detect faces so that we're accurately blurring the face of our person instead of having a set location blurred for an image.
      </p> 
         <div class="image-row">
            <div class="image-container">
               <img src="images/test.jpg" style="width:200px;">
               <figcaption>Face Image</figcaption>
            </div>
            <div class="image-container">
               <img src="images/blurredInput.jpg" style="width:200px;">
               <figcaption>Blurred Face</figcaption>
            </div>
            <div class="image-container">
               <img src="images/generatorRes.jpg" style="width:200px;">
               <figcaption>Generated Hair</figcaption>
            </div>
            <div class="image-container">
               <img src="images/transplantRes.jpg" style="width:200px;">
               <figcaption>Transplanted Face</figcaption>
            </div>
            <div class="image-container">
               <img src="images/finalRes.jpg" style="width:200px;">
               <figcaption>Final Output</figcaption>
            </div>
      </div>
      <h2>Final Results</h2>
      <p>
         We were able to demonstrate a pipeline of what could be done for a GAN powered hairstyle transfer code. However, our results seem to be suffering from our style identifier, resulting in outputs without much variation. Future work would involve fine tuning and adjusting the dataset for the style identifier to produce better results and to identify the weights of the style and reconstruction loss for optimal results. 
      </p>
   </body>
</html>